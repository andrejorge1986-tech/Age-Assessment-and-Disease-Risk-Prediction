{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3e3ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√£o inicial e pastas\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Pastas\n",
    "RAW_FOLDER = r\"C:\\Users\\andr3\\Documents\\DATA SCIENCE\\10794 - Programa√ß√£o avan√ßada com Python\\Age Assessment & Disease Risk Prediction\\src\"\n",
    "CLEAN_FOLDER = os.path.join(os.path.dirname(RAW_FOLDER), \"clean\")\n",
    "os.makedirs(CLEAN_FOLDER, exist_ok=True)\n",
    "# Par√¢metros\n",
    "CHUNK_SIZE = 50000  # n√∫mero de samples por Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db79e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler e checar metadados\n",
    "\n",
    "trainmap_df = pd.read_csv(os.path.join(RAW_FOLDER, \"trainmap.csv\"))\n",
    "trainmap_df['sample_id'] = trainmap_df['sample_id'].astype(str)\n",
    "\n",
    "print(\"Colunas do trainmap:\", trainmap_df.columns)\n",
    "trainmap_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7633e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o de preprocessing (traindata wide ‚Üí long + merge)\n",
    "\n",
    "def process_traindata(file_path):\n",
    "    parquet_files = []\n",
    "    for i, chunk in enumerate(pd.read_csv(file_path, chunksize=1000)):\n",
    "        # Wide ‚Üí long\n",
    "        df_long = chunk.melt(id_vars=\"cpgsite\", var_name=\"sample_id\", value_name=\"value\")\n",
    "        df_long[\"sample_id\"] = df_long[\"sample_id\"].astype(str)\n",
    "        \n",
    "        # Merge com trainmap\n",
    "        df_merged = df_long.merge(trainmap_df, on=\"sample_id\", how=\"left\")\n",
    "        \n",
    "        # Salva em chunks menores para RAM\n",
    "        n_chunks = (len(df_merged) // CHUNK_SIZE) + 1\n",
    "        for j in range(n_chunks):\n",
    "            start = j * CHUNK_SIZE\n",
    "            end = (j + 1) * CHUNK_SIZE\n",
    "            chunk_parquet = df_merged.iloc[start:end]\n",
    "            if len(chunk_parquet) == 0:\n",
    "                continue\n",
    "            file_name = os.path.join(CLEAN_FOLDER, f\"traindata_part_{i:04d}_{j:04d}.parquet\")\n",
    "            chunk_parquet.to_parquet(file_name, index=False)\n",
    "            parquet_files.append(file_name)\n",
    "    return parquet_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96016cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rodar o preprocessing\n",
    "\n",
    "traindata_path = os.path.join(RAW_FOLDER, \"traindata.csv\")\n",
    "parquet_files = process_traindata(traindata_path)\n",
    "print(f\"Parquet files gerados: {len(parquet_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab19c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura de todos os Parquet chunks para an√°lise\n",
    "\n",
    "import glob\n",
    "\n",
    "all_parquets = glob.glob(os.path.join(CLEAN_FOLDER, \"*.parquet\"))\n",
    "df = pd.concat([pd.read_parquet(f) for f in all_parquets], ignore_index=True)\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f6c43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA + Clustering\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# ===== CONFIGURA√á√ÉO =====\n",
    "SRC_DIR = r\"C:\\Users\\andr3\\Documents\\DATA SCIENCE\\10794 - Programa√ß√£o avan√ßada com Python\\Age Assessment & Disease Risk Prediction\\src\"\n",
    "CLEAN_FOLDER = os.path.join(SRC_DIR, \"clean\")\n",
    "os.makedirs(CLEAN_FOLDER, exist_ok=True)\n",
    "\n",
    "TRAIN_FILE = os.path.join(SRC_DIR, \"traindata.csv\")\n",
    "TRAINMAP_FILE = os.path.join(SRC_DIR, \"trainmap.csv\")\n",
    "OUTPUT_FILE = os.path.join(CLEAN_FOLDER, \"train_processed.parquet\")\n",
    "\n",
    "# ===== CARREGAR TRAINMAP =====\n",
    "trainmap_df = pd.read_csv(TRAINMAP_FILE, usecols=[\"age\", \"gender\", \"sample_type\", \"disease\"])\n",
    "trainmap_df[\"sample_id\"] = trainmap_df.index.astype(str)\n",
    "\n",
    "# ===== PARAMETROS =====\n",
    "CHUNK_SIZE = 10000  # menor para reduzir uso de RAM\n",
    "N_COMPONENTS = 2\n",
    "N_CLUSTERS = 4\n",
    "\n",
    "# ===== Inicializar PCA e KMeans =====\n",
    "pca = IncrementalPCA(n_components=N_COMPONENTS)\n",
    "kmeans = MiniBatchKMeans(n_clusters=N_CLUSTERS, random_state=42, batch_size=500)\n",
    "\n",
    "# ===== PRIMEIRA PASSAGEM: Treinar PCA =====\n",
    "print(\"Treinando PCA incrementalmente...\")\n",
    "for chunk in pd.read_csv(TRAIN_FILE, chunksize=CHUNK_SIZE, usecols=lambda c: c.startswith(\"train\")):\n",
    "    chunk = chunk.fillna(0).astype(\"float32\")\n",
    "    pca.partial_fit(chunk)\n",
    "print(\"‚úÖ PCA treinado!\")\n",
    "\n",
    "# ===== SEGUNDA PASSAGEM: Transformar e clusterizar =====\n",
    "print(\"Aplicando PCA e KMeans em blocos...\")\n",
    "results = []\n",
    "sample_counter = 0\n",
    "\n",
    "for chunk in pd.read_csv(TRAIN_FILE, chunksize=CHUNK_SIZE):\n",
    "    feature_cols = [c for c in chunk.columns if c.startswith(\"train\")]\n",
    "    chunk_features = chunk[feature_cols].fillna(0).astype(\"float32\")\n",
    "\n",
    "    # sample_id incremental\n",
    "    chunk[\"sample_id\"] = np.arange(sample_counter, sample_counter + len(chunk)).astype(str)\n",
    "    sample_counter += len(chunk)\n",
    "\n",
    "    # PCA transform\n",
    "    pca_result = pca.transform(chunk_features)\n",
    "    chunk[\"PC1\"] = pca_result[:, 0]\n",
    "    chunk[\"PC2\"] = pca_result[:, 1]\n",
    "\n",
    "    # Clustering\n",
    "    cluster_labels = kmeans.partial_fit(chunk_features).predict(chunk_features)\n",
    "    chunk[\"cluster\"] = cluster_labels\n",
    "\n",
    "    # Selecionar colunas relevantes\n",
    "    results.append(chunk[[\"sample_id\", \"PC1\", \"PC2\", \"cluster\"]])\n",
    "    del chunk, chunk_features, pca_result, cluster_labels  # liberar mem√≥ria\n",
    "\n",
    "# ===== JUNTAR TODOS E MERGE =====\n",
    "final_df = pd.concat(results, ignore_index=True)\n",
    "final_df = final_df.merge(trainmap_df, on=\"sample_id\", how=\"left\")\n",
    "\n",
    "# ===== SALVAR PARQUET =====\n",
    "final_df.to_parquet(OUTPUT_FILE, index=False)\n",
    "print(\"‚úÖ Arquivo processado salvo em:\", OUTPUT_FILE)\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cade9542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estrutura sugerida para modelo supervisionado\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "# ===== CONFIGURA√á√ÉO DE CAMINHOS =====\n",
    "SRC_DIR = r\"C:\\Users\\andr3\\Documents\\DATA SCIENCE\\10794 - Programa√ß√£o avan√ßada com Python\\Age Assessment & Disease Risk Prediction\\src\"\n",
    "CLEAN_FOLDER = os.path.join(SRC_DIR, \"clean\")\n",
    "PARQUET_FILE = os.path.join(CLEAN_FOLDER, \"train_processed.parquet\")\n",
    "MODEL_PATH = os.path.join(CLEAN_FOLDER, \"model_sgd.pkl\")\n",
    "OUTPUT_PARQUET = os.path.join(CLEAN_FOLDER, \"train_with_predictions.parquet\")\n",
    "\n",
    "# ===== CARREGAR DADOS =====\n",
    "df = pd.read_parquet(PARQUET_FILE)\n",
    "\n",
    "# ===== FILTRAR REGISTOS V√ÅLIDOS =====\n",
    "df = df.dropna(subset=[\"disease\", \"PC1\", \"PC2\"])\n",
    "X = df[[\"PC1\", \"PC2\"]].astype(\"float32\")\n",
    "\n",
    "# ===== CODIFICAR TARGET =====\n",
    "le = LabelEncoder()\n",
    "df[\"target\"] = le.fit_transform(df[\"disease\"].astype(str))\n",
    "y = df[\"target\"].astype(\"int32\")\n",
    "classes = np.unique(y)\n",
    "\n",
    "# ===== INICIALIZAR MODELO INCREMENTAL =====\n",
    "model = SGDClassifier(loss=\"log_loss\", random_state=42)\n",
    "\n",
    "# ===== TREINO EM MINI-BATCHES =====\n",
    "BATCH_SIZE = 1000\n",
    "for i in range(0, len(X), BATCH_SIZE):\n",
    "    X_batch = X.iloc[i:i + BATCH_SIZE]\n",
    "    y_batch = y.iloc[i:i + BATCH_SIZE]\n",
    "    model.partial_fit(X_batch, y_batch, classes=classes)\n",
    "\n",
    "# ===== SALVAR MODELO TREINADO =====\n",
    "joblib.dump(model, MODEL_PATH)\n",
    "print(f\"‚úÖ Modelo supervisionado salvo em: {MODEL_PATH}\")\n",
    "\n",
    "# ===== PREVIS√ïES =====\n",
    "y_pred = model.predict(X)\n",
    "if len(y_pred) != len(df):\n",
    "    raise ValueError(\"N√∫mero de previs√µes n√£o corresponde ao n√∫mero de amostras.\")\n",
    "\n",
    "# ===== DECODIFICAR PREVIS√ïES =====\n",
    "df[\"predicted_disease\"] = le.inverse_transform(y_pred)\n",
    "\n",
    "# ===== AVALIA√á√ÉO DO MODELO =====\n",
    "df_eval = df.dropna(subset=[\"disease\", \"predicted_disease\"])\n",
    "print(\"\\nüìä Relat√≥rio de Classifica√ß√£o:\")\n",
    "print(classification_report(df_eval[\"disease\"], df_eval[\"predicted_disease\"]))\n",
    "\n",
    "# ===== MATRIZ DE CONFUS√ÉO =====\n",
    "cm = confusion_matrix(df_eval[\"disease\"], df_eval[\"predicted_disease\"], labels=le.classes_)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=le.classes_, yticklabels=le.classes_, cmap=\"Blues\")\n",
    "plt.title(\"Matriz de Confus√£o - Previs√£o de Doen√ßa\")\n",
    "plt.xlabel(\"Predito\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===== EXPORTAR PARQUET COM PREVIS√ïES =====\n",
    "df.to_parquet(OUTPUT_PARQUET, index=False)\n",
    "print(f\"‚úÖ Dados com previs√µes salvos em: {OUTPUT_PARQUET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7f8898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo com RandomForestClassifier\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "# ===== CONFIGURA√á√ÉO =====\n",
    "SRC_DIR = r\"C:\\Users\\andr3\\Documents\\DATA SCIENCE\\10794 - Programa√ß√£o avan√ßada com Python\\Age Assessment & Disease Risk Prediction\\src\"\n",
    "CLEAN_FOLDER = os.path.join(SRC_DIR, \"clean\")\n",
    "PARQUET_FILE = os.path.join(CLEAN_FOLDER, \"train_processed.parquet\")\n",
    "MODEL_PATH = os.path.join(CLEAN_FOLDER, \"model_rf.pkl\")\n",
    "OUTPUT_PARQUET = os.path.join(CLEAN_FOLDER, \"train_rf_predictions.parquet\")\n",
    "\n",
    "# ===== CARREGAR DADOS =====\n",
    "df = pd.read_parquet(PARQUET_FILE)\n",
    "df = df.dropna(subset=[\"disease\", \"PC1\", \"PC2\"])\n",
    "\n",
    "# ===== FEATURES E TARGET =====\n",
    "X = df[[\"PC1\", \"PC2\"]]\n",
    "le = LabelEncoder()\n",
    "df[\"target\"] = le.fit_transform(df[\"disease\"].astype(str))\n",
    "y = df[\"target\"]\n",
    "\n",
    "# ===== TREINAR MODELO =====\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# ===== SALVAR MODELO =====\n",
    "joblib.dump(model, MODEL_PATH)\n",
    "print(f\"‚úÖ Modelo Random Forest salvo em: {MODEL_PATH}\")\n",
    "\n",
    "# ===== PREVIS√ïES =====\n",
    "y_pred = model.predict(X)\n",
    "df[\"predicted_disease\"] = le.inverse_transform(y_pred)\n",
    "\n",
    "# ===== AVALIA√á√ÉO =====\n",
    "print(\"\\nüìä Relat√≥rio de Classifica√ß√£o:\")\n",
    "print(classification_report(df[\"disease\"], df[\"predicted_disease\"]))\n",
    "\n",
    "cm = confusion_matrix(df[\"disease\"], df[\"predicted_disease\"], labels=le.classes_)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=le.classes_, yticklabels=le.classes_, cmap=\"Greens\")\n",
    "plt.title(\"Matriz de Confus√£o - Random Forest\")\n",
    "plt.xlabel(\"Predito\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===== EXPORTAR PARQUET COM PREVIS√ïES =====\n",
    "df.to_parquet(OUTPUT_PARQUET, index=False)\n",
    "print(f\"‚úÖ Dados com previs√µes salvos em: {OUTPUT_PARQUET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d760a392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparativo em C√≥digo: SGD vs Random ForesT\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# ===== CARREGAR PREVIS√ïES =====\n",
    "SRC_DIR = r\"C:\\Users\\andr3\\Documents\\DATA SCIENCE\\10794 - Programa√ß√£o avan√ßada com Python\\Age Assessment & Disease Risk Prediction\\src\"\n",
    "CLEAN_FOLDER = os.path.join(SRC_DIR, \"clean\")\n",
    "\n",
    "df_sgd = pd.read_parquet(os.path.join(CLEAN_FOLDER, \"train_with_predictions.parquet\"))\n",
    "df_rf = pd.read_parquet(os.path.join(CLEAN_FOLDER, \"train_rf_predictions.parquet\"))\n",
    "\n",
    "# ===== RELAT√ìRIO DE CLASSIFICA√á√ÉO =====\n",
    "print(\"üìä SGDClassifier:\")\n",
    "print(classification_report(df_sgd[\"disease\"], df_sgd[\"predicted_disease\"]))\n",
    "\n",
    "print(\"\\nüìä RandomForestClassifier:\")\n",
    "print(classification_report(df_rf[\"disease\"], df_rf[\"predicted_disease\"]))\n",
    "\n",
    "# ===== MATRIZES DE CONFUS√ÉO LADO A LADO =====\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "cm_sgd = confusion_matrix(df_sgd[\"disease\"], df_sgd[\"predicted_disease\"], labels=df_sgd[\"disease\"].unique())\n",
    "sns.heatmap(cm_sgd, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[0])\n",
    "axes[0].set_title(\"SGDClassifier\")\n",
    "axes[0].set_xlabel(\"Predito\")\n",
    "axes[0].set_ylabel(\"Real\")\n",
    "\n",
    "cm_rf = confusion_matrix(df_rf[\"disease\"], df_rf[\"predicted_disease\"], labels=df_rf[\"disease\"].unique())\n",
    "sns.heatmap(cm_rf, annot=True, fmt=\"d\", cmap=\"Greens\", ax=axes[1])\n",
    "axes[1].set_title(\"RandomForestClassifier\")\n",
    "axes[1].set_xlabel(\"Predito\")\n",
    "axes[1].set_ylabel(\"Real\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1beac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DASHBOARD DE CLUSTERS - AVALIA√á√ÉO BIOL√ìGICA\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ===== CONFIGURA√á√ÉO =====\n",
    "SRC_DIR = r\"C:\\Users\\andr3\\Documents\\DATA SCIENCE\\10794 - Programa√ß√£o avan√ßada com Python\\Age Assessment & Disease Risk Prediction\\src\"\n",
    "CLEAN_FOLDER = os.path.join(SRC_DIR, \"clean\")\n",
    "PARQUET_FILE = os.path.join(CLEAN_FOLDER, \"train_processed.parquet\")\n",
    "\n",
    "# ===== CARREGAR DADOS =====\n",
    "if not os.path.exists(PARQUET_FILE):\n",
    "    raise FileNotFoundError(f\"Nenhum arquivo encontrado em {PARQUET_FILE}\")\n",
    "\n",
    "df = pd.read_parquet(PARQUET_FILE)\n",
    "\n",
    "# ===== APLICAR FILTROS MANUALMENTE =====\n",
    "idade = (20, 60)\n",
    "genero = df[\"gender\"].dropna().unique().tolist()\n",
    "tipo = df[\"sample_type\"].dropna().unique().tolist()\n",
    "doenca = df[\"disease\"].dropna().unique().tolist()\n",
    "\n",
    "df_filtrado = df[\n",
    "    (df[\"age\"].between(idade[0], idade[1])) &\n",
    "    (df[\"gender\"].isin(genero)) &\n",
    "    (df[\"sample_type\"].isin(tipo)) &\n",
    "    (df[\"disease\"].isin(doenca))\n",
    "]\n",
    "\n",
    "# ===== GR√ÅFICOS INTERATIVOS =====\n",
    "px.scatter(df_filtrado, x=\"PC1\", y=\"PC2\", color=\"cluster\",\n",
    "           hover_data=[\"sample_id\", \"age\", \"gender\", \"sample_type\", \"disease\"],\n",
    "           title=\"Clusters no Espa√ßo PCA\").show()\n",
    "\n",
    "px.scatter(df_filtrado, x=\"PC1\", y=\"PC2\", color=\"gender\", symbol=\"cluster\",\n",
    "           hover_data=[\"sample_id\", \"age\", \"sample_type\", \"disease\"],\n",
    "           title=\"Distribui√ß√£o PCA por G√™nero\").show()\n",
    "\n",
    "doenca_cluster = df_filtrado.groupby([\"cluster\", \"disease\"]).size().reset_index(name=\"n\")\n",
    "px.bar(doenca_cluster, x=\"cluster\", y=\"n\", color=\"disease\",\n",
    "       title=\"Distribui√ß√£o de Doen√ßas por Cluster\", barmode=\"stack\").show()\n",
    "\n",
    "px.box(df_filtrado, x=\"cluster\", y=\"age\", color=\"cluster\",\n",
    "       title=\"Distribui√ß√£o de Idade por Cluster\").show()\n",
    "\n",
    "# ===== HEATMAP DE CORRELA√á√ÉO =====\n",
    "df_corr = df_filtrado.copy()\n",
    "for col in [\"gender\", \"sample_type\", \"disease\"]:\n",
    "    df_corr[col] = df_corr[col].astype(\"category\").cat.codes\n",
    "\n",
    "corr_matrix = df_corr[[\"age\", \"gender\", \"sample_type\", \"disease\", \"cluster\"]].corr()\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correla√ß√£o entre Vari√°veis\")\n",
    "plt.show()\n",
    "\n",
    "# ===== CONTAGEM POR CLUSTER =====\n",
    "count_df = df_filtrado[\"cluster\"].value_counts().rename(\"n_amostras\").reset_index().rename(columns={\"index\": \"cluster\"})\n",
    "print(\"üìä Contagem por Cluster\")\n",
    "print(count_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
